import streamlit as st 
import os
from datetime import datetime
import time
import os
from datetime import datetime
from typing import Dict, List, Tuple, Any
import chromadb
from chromadb.utils import embedding_functions
import streamlit as st
import dashscope  # æ›¿æ¢ OpenAI
from dashscope import Generation  # æ›¿æ¢ OpenAI
import requests
from duckduckgo_search import DDGS
import numpy as np
import re
from FlagEmbedding import BGEM3FlagModel

class FactChecker:
    
    def __init__(self, api_base: str, model: str, temperature: float, max_tokens: int):
        """
        åˆå§‹åŒ–äº‹å®æ ¸æŸ¥å™¨ï¼Œè®¾ç½®é…ç½®å‚æ•°
        
        å‚æ•°:
            api_base: LLM APIçš„åŸºç¡€URLï¼ˆæœªä½¿ç”¨ï¼Œå·²å¼ƒç”¨ï¼‰
            model: ç”¨äºäº‹å®æ ¸æŸ¥çš„æ¨¡å‹ï¼ˆéœ€æ›¿æ¢ä¸º DashScope æ”¯æŒçš„æ¨¡å‹åç§°ï¼‰
            temperature: LLMçš„æ¸©åº¦å‚æ•°
            max_tokens: LLMå“åº”çš„æœ€å¤§tokenæ•°
        """
        # âœ… è®¾ç½® DashScope API Keyï¼ˆéœ€æ›¿æ¢ä¸ºä½ çš„é˜¿é‡Œäº‘ API Keyï¼‰
        self.dashscope_api_key = "sk-8f7cf717823e4a718c0b434b74947478"  # æ›¿æ¢ä¸ºä½ çš„é˜¿é‡Œäº‘ API Key
        dashscope.api_key = self.dashscope_api_key
        
        self.model = model  # ä½¿ç”¨ DashScope æ”¯æŒçš„æ¨¡å‹åç§°ï¼ˆå¦‚ qwen2-7b-instruct-ft-202505022309-1958ï¼‰
        self.temperature = temperature
        self.max_tokens = max_tokens
        
        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        try:
            self.embedding_model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=True)
        except Exception as e:
            st.error(f"åŠ è½½BGE-M3æ¨¡å‹é”™è¯¯: {str(e)}")
            self.embedding_model = None
            
        # åˆå§‹åŒ–Chromaæœ¬åœ°çŸ¥è¯†åº“
        try:
            self.chroma_client = chromadb.Client()
            self.collection = self.chroma_client.get_or_create_collection(
                name="my-knowledge-base", 
                embedding_function=embedding_functions.SentenceTransformerEmbeddingFunction(model_name="BAAI/bge-m3")
            )
        except Exception as e:
            st.error(f"åŠ è½½Chromaé”™è¯¯: {str(e)}")
            self.collection = None
            
    def extract_claim(self, text: str) -> str:
        """
        Extract core claims from the input text using LLM.
        
        Args:
            text: The input text to extract claims from
            
        Returns:
            extracted claim
        """
        system_prompt = """
        You are a precise claim extraction assistant. Analyze the provided news and summarize the central idea of it.
        Format the central idea as a worthy-check statement, which is a claim that can be verified independently.
        output format:
        claim: <claim>
        """
        #You are a precise claim extraction assistant. Analyze the provided news and summarize the central idea of it.
        #Format the central idea as a worthy-check statement, which is a claim that can be verified independently.
        #output format:
        #claim: <claim>
        try:
            # âœ… ä½¿ç”¨ DashScope çš„ Generation.call æ›¿ä»£ OpenAI
            prompt = f"{system_prompt}\n\nç”¨æˆ·è¾“å…¥: {text}"
            response = Generation.call(
                model=self.model,
                prompt=prompt,
                temperature=0.0,
                max_tokens=1000
            )
            
            claims_text = response.output.text
            
            # å°†ç¼–å·åˆ—è¡¨è§£æä¸ºå•ç‹¬çš„å£°æ˜
            claims = re.findall(r'\d+\.\s+(.*?)(?=\n\d+\.|\Z)', claims_text, re.DOTALL)
            
            # æ¸…ç†å£°æ˜
            claims = [claim.strip() for claim in claims if claim.strip()]
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç¼–å·å£°æ˜ï¼Œåˆ™æŒ‰æ¢è¡Œç¬¦åˆ†å‰²
            if not claims and claims_text.strip():
                claims = [line.strip() for line in claims_text.strip().split('\n') if line.strip()]
            
            return claims[0] if claims else text
            
        except Exception as e:
            st.error(f"æå–å£°æ˜é”™è¯¯: {str(e)}")
            return text  # ä½œä¸ºå›é€€è¿”å›åŸå§‹æ–‡æœ¬

    def extract_keyinformation(self, text: str) -> str:
        """
        ä½¿ç”¨LLMä»è¾“å…¥æ–‡æœ¬ä¸­æå–æ ¸å¿ƒå£°æ˜åŠå…¶ä¸Šä¸‹æ–‡ä¿¡æ¯
    
        å‚æ•°:
            text: è¦ä»ä¸­æå–å£°æ˜çš„è¾“å…¥æ–‡æœ¬
        
        è¿”å›:
            æå–çš„ç»“æ„åŒ–ä¿¡æ¯å­—ç¬¦ä¸²
        """
        system_prompt = """
        You are a precise claim extraction assistant. Please perform the following multi-level analysis on the user input:
    
        1. Identify all entities involved in the text (person/organization/location/time) and their types
        2. Determine the primary event category (political/economic/social/technological etc.)
        3. Extract key characteristics (source reliability/evidence strength/timeliness)
        4. Summarize the central idea and convert it into a verifiable claim
    
        Output format requirements:
        claim: <Verifiable complete statement>
        entities: <Entity type1:Entity name1, Entity type2:Entity name2...>
        event_type: <Primary event category>
        key_features: <Feature1:Value1, Feature2:Value2...>
    
        Note: Please strictly follow the format and avoid additional explanations
       """
    
        try:
        # âœ… ä½¿ç”¨ DashScope çš„ Generation.call æ›¿ä»£ OpenAI
          prompt = f"{system_prompt}\n\nç”¨æˆ·è¾“å…¥: {text}"
          response = Generation.call(
            model=self.model,
            prompt=prompt,
            temperature=0.0,
            max_tokens=1000
         )
        
          raw_output = response.output.text
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ç²¾ç¡®æå–å„å­—æ®µ
          claim_match = re.search(r'claim:\s*(.*?)(?=\n\S+:|\Z)', raw_output, re.DOTALL)
          entities_match = re.search(r'entities:\s*(.*?)(?=\n\S+:|\Z)', raw_output, re.DOTALL)
          event_type_match = re.search(r'event_type:\s*(.*?)(?=\n\S+:|\Z)', raw_output, re.DOTALL)
          key_features_match = re.search(r'key_features:\s*(.*?)(?=\n\S+:|\Z)', raw_output, re.DOTALL)
        
        # æ¸…ç†æå–çš„å­—æ®µå†…å®¹
          claim = claim_match.group(1).strip() if claim_match else "æœªæå–åˆ°æœ‰æ•ˆå£°æ˜"
          entities = entities_match.group(1).strip() if entities_match else "æ— å®ä½“ä¿¡æ¯"
          event_type = event_type_match.group(1).strip() if event_type_match else "æœªçŸ¥äº‹ä»¶ç±»å‹"
          key_features = key_features_match.group(1).strip() if key_features_match else "æ— ç‰¹å¾ä¿¡æ¯"
        
        # æ ¼å¼åŒ–è¾“å‡º
          formatted_output = (
            f"claim: {claim}\n"
            f"entities: {entities}\n"
            f"event_type: {event_type}\n"
            f"key_features: {key_features}"
          )
        
          return formatted_output
            
        except Exception as e:
          st.error(f"æå–å£°æ˜é”™è¯¯: {str(e)}")
        return text  # ä½œä¸ºå›é€€è¿”å›åŸå§‹æ–‡æœ¬
    
    def search_local_knowledge(self, claim: str, top_k: int = 5) -> List[Dict[str, str]]:
        """
        Search for evidence from local Chroma knowledge base.

        Args:
            claim: The claim to search for
            top_k: Number of top results to return

        Returns:
               List of evidence documents
        """
        if not self.collection:
            return []
    
        try:
            results = self.collection.query(
                query_texts=[claim],
                n_results=top_k
            )
            evidence_docs = []
            for text, metadata in zip(results["documents"][0], results["metadatas"][0]):
                evidence_docs.append({
                   'title': metadata.get('title', ''),
                    'url': metadata.get('url', 'Local Knowledge Base'),
                    'snippet': text
            })
            return evidence_docs
        except Exception as e:
            st.error(f"Error searching local knowledge base: {str(e)}")
            return []
        
    def search_evidence(self, claim: str, num_results: int = 5) -> List[Dict[str, str]]:
        """
        Search for evidence using DuckDuckGo.
        
        Args:
            claim: The claim to search for evidence
            num_results: Number of search results to return
            
        Returns:
            List of evidence documents with title, url, and snippet
        """
        try:
            ddgs = DDGS(timeout=60)#proxy="socks5://127.0.0.1:20170",
            results = list(ddgs.text(claim, max_results=num_results))
            
            external_evidence = []#æ­¤å¤„å°†evidence_docsæ”¹ä¸ºexternal_evidence
            for result in results:
                #æ­¤å¤„å°†evidence_docsæ”¹ä¸ºexternal_evidence
                external_evidence.append({      
                    'title': result.get('title', ''),
                    'url': result.get('href', ''),
                    'snippet': result.get('body', '')
                })
            # 2. æœ¬åœ°çŸ¥è¯†åº“æ£€ç´¢
            local_evidence = self.search_local_knowledge(claim, top_k=num_results)
            # 3. åˆå¹¶ä¸¤ç§æ¥æº
            combined_evidence = external_evidence + local_evidence
            return combined_evidence
            #return evidence_docs
        except Exception as e:
            st.error(f"Error searching for evidence: {str(e)}")
            return []

    #ä¸‹é¢evidence_docsæ”¹ä¸ºexternal_evidence
    def get_evidence_chunks(self,evidence_docs: List[Dict[str, str]], claim: str, chunk_size: int = 200, chunk_overlap: int = 50, top_k: int = 10) -> List[Dict[str, Any]]:
        """
        Extract and rank evidence chunks related to the claim using BGE-M3.
        
        Args:
            evidence_docs: List of evidence documents
            claim: The claim to match with evidence
            chunk_size: Size of text chunks in characters
            chunk_overlap: Overlap between chunks in characters
            top_k: Number of top chunks to return
            
        Returns:
            List of ranked evidence chunks with similarity scores
        """
        if not self.embedding_model:
            return [{
                'text': "Evidence ranking unavailable - BGE-M3 model could not be loaded.",
                'source': "System",
                'similarity': 0.0
            }]
        
        #æ£€æŸ¥ evidence_docs æ˜¯å¦ä¸ºç©ºåœ¨å¤„ç†è¯æ®ç‰‡æ®µä¹‹å‰ï¼Œç¡®ä¿ evidence_docs åˆ—è¡¨ä¸æ˜¯ç©ºçš„ã€‚
        #å¦‚æœåœ¨æœç´¢è¿‡ç¨‹ä¸­æ²¡æœ‰æ‰¾åˆ°è¯æ®ï¼Œéœ€è¦ä¼˜é›…åœ°å¤„ç†è¿™ç§æƒ…å†µï¼Œå¯ä»¥è¿”å›ä¸€ä¸ªé»˜è®¤æ¶ˆæ¯æˆ–è·³è¿‡ç‰‡æ®µå¤„ç†ã€‚
        if not evidence_docs:  # æ£€æŸ¥æ˜¯å¦æ²¡æœ‰æ‰¾åˆ°è¯æ®
            return [{
            'text': "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³è¯æ®ã€‚",
            'source': "ç³»ç»Ÿ",
            'similarity': 0.0
            }]
        
        try:
            # Create text chunks from evidence documents
            all_chunks = []
            
            for doc in evidence_docs:
                # Add title as a separate chunk
                all_chunks.append({
                    'text': doc['title'],
                    'source': doc['url'],
                })
                
                # Process the snippet into overlapping chunks
                snippet = doc['snippet']
                if len(snippet) <= chunk_size:
                    # If snippet is shorter than chunk_size, use it as is
                    all_chunks.append({
                        'text': snippet,
                        'source': doc['url'],
                    })
                else:
                    # Create overlapping chunks
                    for i in range(0, len(snippet), chunk_size - chunk_overlap):
                        chunk_text = snippet[i:i + chunk_size]
                        if len(chunk_text) >= chunk_size // 2:  # Only keep chunks of reasonable size
                            all_chunks.append({
                                'text': chunk_text,
                                'source': doc['url'],
                            })
            
            # Compute embeddings for claim
            claim_embedding = self.embedding_model.encode(claim)['dense_vecs']
            
            # Compute embeddings for chunks
            chunk_texts = [chunk['text'] for chunk in all_chunks]
            chunk_embeddings = self.embedding_model.encode(chunk_texts)['dense_vecs']
            
            # Calculate similarities
            similarities = []
            for i, chunk_embedding in enumerate(chunk_embeddings):
                similarity = np.dot(claim_embedding, chunk_embedding) / (
                    np.linalg.norm(claim_embedding) * np.linalg.norm(chunk_embedding)
                )
                similarities.append(float(similarity))
            
            # Add similarities to chunks
            for i, similarity in enumerate(similarities):
                all_chunks[i]['similarity'] = similarity
            
            # Sort chunks by similarity (descending)
            ranked_chunks = sorted(all_chunks, key=lambda x: x['similarity'], reverse=True)
            
            # Return top k chunks
            return ranked_chunks[:top_k]
            
        except Exception as e:
            st.error(f"Error ranking evidence: {str(e)}")
            return [{
                'text': f"Error ranking evidence: {str(e)}",
                'source': "System",
                'similarity': 0.0
            }]
    def evaluate_claim(self, keyinformation: str, claim: str, evidence_chunks: List[Dict[str, Any]]) -> Dict[str, str]:
        """
        Evaluate the truthfulness of a claim based on evidence using LLM.
        
        Args:
            claim: The claim to evaluate
            evidence_chunks: The evidence chunks to use for evaluation
            
        Returns:
            Dictionary with verdict and reasoning
        """
        system_prompt = """
        [Role] Fake News Verifier  
[Task] Strictly follow this analysis protocol:  

1. **Source Credibility Verification (Weight: 40%)**  
   âœ“ Official sources (government/academic/professional journals) â†’ 5â˜…  
   âœ“ Mainstream media (AFP/Xinhua, etc.) â†’ 3â˜…  
   âœ— Personal blogs/anonymous platforms â†’ 1â˜…  

2. **Evidence Quality Assessment (Weight: 40%)**  
   â”‚ Direct Evidence      â”‚ Indirect Evidence  
   â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  
   â”‚ Experimental data   â”‚ Expert testimony  
   â”‚ Statistical reports â”‚ Historical cases  
   â”‚ On-site footage     â”‚ Literature citations  

3. **Logical Completeness Review (Weight: 20%)**  
   - Statistical trap detection: Sample size <30  
   - Causality fallacy identification: Aâ†’B misrepresented as Bâ†’A  
   - Temporal paradox verification: Event interval <2h  

4. **Binary Judgment Criteria**  
   **TRUE**: Total weight â‰¥70% with no core contradictions  
   **FALSE**: Existence of counter-evidence chains/multiple logical flaws or majority sourcing from personal blogs/anonymous platforms  

[Output Specification]  
**VERDICT**: [TRUE|FALSE|PARTIALLY TRUE]  
**EVIDENCE WEIGHT**: [Total evidence weight percentage]  
**REASONING**:  
1. Source rating: â–¡Official â–¡Verified media â–¡Suspicious platform  
2. Key evidence: [E-12] NASA remote sensing data (2024Q3)  
3. Logical flaw: Sample selection bias detected (p=0.12)   
4. Temporal validity: âœ“ Latest satellite imagery (2025-03)  

**Input**: "There's an alien base on the far side of the Moon"  
**Output**:  
VERDICT: FALSE  
EVIDENCE WEIGHT: 92%  
REASONING:  
1. Source rating: NASA official data (Weight: 50%)  
2. Key evidence: [E-3] Lunar rover scans show no anomalous structures  
3. Logical flaw: Eyewitness accounts constitute anecdotal fallacy  
4. Temporal verification: Latest lunar mission data (2024Q3)  
        """
    
        # Prepare evidence text for the prompt
        evidence_text = "\n\n".join([
            f"evidence {i+1} (relevance: {chunk['similarity']:.2f}):\n{chunk['text']}\nsource: {chunk['source']}"
            for i, chunk in enumerate(evidence_chunks)
        ])

        try:
            prompt = f"{system_prompt}\n\nkeyinformation: {keyinformation}\n\ntext:{claim}\n\nevidence:\n{evidence_text}"
            response = Generation.call(
                model=self.model,
                prompt=prompt,
                temperature=self.temperature,
                max_tokens=self.max_tokens
            )
            
            result_text = response.output.text
            # æ¸…ç†ç‰¹æ®Šç©ºç™½å­—ç¬¦ï¼ˆå¤„ç†å…¨è§’ç©ºæ ¼/é›¶å®½ç©ºæ ¼/æ¢è¡Œï¼‰
            result_text_clean = re.sub(r'[\u3000\u200B\xa0]+', ' ', result_text)
            result_text_clean = re.sub(r'\s+', ' ', result_text_clean)  # ç»Ÿä¸€ç©ºç™½æ ¼å¼
            
            verdict_match = re.search(r'\s*(TRUE|FALSE|PARTIALLY TRUE).*?$', result_text, re.IGNORECASE | re.MULTILINE)
            verdict = verdict_match.group(1) if verdict_match else "UNVERIFIABLE"
            
            reasoning_match = re.search(r'REASONING:\s*(.*)', result_text, re.DOTALL)
            reasoning = reasoning_match.group(1).strip() if reasoning_match else result_text
            
            return {
                "verdict": verdict,
                "reasoning": reasoning
            }
            
        except Exception as e:
            st.error(f"è¯„ä¼°å£°æ˜é”™è¯¯: {str(e)}")
            return {
                "verdict": "é”™è¯¯",
                "reasoning": f"è¯„ä¼°è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"
            }
            
        except Exception as e:
            st.error(f"Error evaluating claim: {str(e)}")
            return {
                "verdict": "ERROR",
                "reasoning": f"An error occurred during evaluation: {str(e)}"
            }

    def check_fact(self, text: str) -> Dict[str, Any]:
        """
        Main function to check the factuality of a statement.
        
        Args:
            text: The statement to fact-check
            
        Returns:
            Dictionary with all results of the fact-checking process
        """
        # 1. Extract core claim
        claim = self.extract_claim(text)
        
        result = {
            "original_text": text,
            "claim": claim,
            "results": []
        }
        
        # æœé›†å…³é”®ä¿¡æ¯
        key=self.extract_keyinformation(text)
        
        # 2. Search for evidence
        evidence_docs = self.search_evidence(claim)
        
        # 3. Get relevant evidence chunks
        evidence_chunks = self.get_evidence_chunks(evidence_docs, claim)
        
        # 4. Evaluate claim based on evidence
        evaluation = self.evaluate_claim(key, claim, evidence_chunks)
        
        # Add results for this claim
        result={
            "claim": claim,
            "evidence_docs": evidence_docs,
            "evidence_chunks": evidence_chunks,
            "verdict": evaluation["verdict"],
            "reasoning": evaluation["reasoning"]
        }
        
        return result


# Function to be imported in the main Streamlit app
def check_fact(claim: str, api_base: str, model: str, temperature: float, max_tokens: int) -> Dict[str, Any]:
    """
    Public interface for fact checking to be used by the Streamlit app.
    
    Args:
        claim: The statement to fact-check
        api_base: The base URL for the LLM API
        model: The model to use for fact checking
        temperature: Temperature parameter for LLM
        max_tokens: Maximum tokens for LLM response
        
    Returns:
        Dictionary with all results of the fact-checking process
    """
    checker = FactChecker(api_base, model, temperature, max_tokens)
    return checker.check_fact(claim)

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="AIè™šå‡æ–°é—»æ£€æµ‹å™¨",
    page_icon="ğŸ”",
    layout="wide",
    menu_items={
        'Get Help': None,
        'Report a bug': None,
        'About': None
    }
)

# åº”ç”¨æ ‡é¢˜å’Œæè¿°
st.title("AIè™šå‡æ–°é—»æ£€æµ‹å™¨")
st.markdown("""
æœ¬åº”ç”¨ç¨‹åºä½¿ç”¨APIè°ƒç”¨LLMéªŒè¯é™ˆè¿°çš„å‡†ç¡®æ€§ã€‚
è¯·åœ¨ä¸‹æ–¹è¾“å…¥éœ€è¦æ ¸æŸ¥çš„æ–°é—»ï¼Œç³»ç»Ÿå°†æ£€ç´¢ç½‘ç»œè¯æ®è¿›è¡Œæ–°é—»æ ¸æŸ¥ï¼Œæ— ç½‘ç»œæ—¶å°†åªè¿›è¡Œæœ¬åœ°çŸ¥è¯†åº“æ£€ç´¢ã€‚
""")

# ä¾§è¾¹æ é…ç½®
with st.sidebar:
    st.header("é…ç½®")
    
    # æœ¬åœ°APIç«¯ç‚¹è®¾ç½®
    api_base = st.text_input("APIåŸºç¡€URL", value="https://dashscope.aliyuncs.com/compatible-mode/v1", 
                             help="æ‚¨çš„æœ¬åœ°APIç«¯ç‚¹çš„åŸºç¡€URL")
    
    # æ¨¡å‹é€‰æ‹© - å·²æ›´æ–°ä¸ºæœ¬åœ°Qwenæ¨¡å‹
    model_option = st.selectbox(
        "é€‰æ‹©æ¨¡å‹",
        ["qwen2.5-14b-instruct-1m"],  # ä½ å¯ä»¥æ‰©å±•æ›´å¤šæ¨¡å‹é€‰é¡¹
        index=0,
        help="ä½¿ç”¨æœ¬åœ°Qwen2.5æ¨¡å‹"
    )
    
    # é«˜çº§è®¾ç½®æŠ˜å éƒ¨åˆ†
    with st.expander("é«˜çº§è®¾ç½®"):
        temperature = st.slider("æ¸©åº¦", min_value=0.0, max_value=1.0, value=0.0, step=0.1, 
                               help="è¾ƒä½çš„å€¼ä½¿å“åº”æ›´ç¡®å®šï¼Œè¾ƒé«˜çš„å€¼ä½¿å“åº”æ›´å…·åˆ›é€ æ€§")
        max_tokens = st.slider("æœ€å¤§å“åº”é•¿åº¦", min_value=100, max_value=8000, value=1000, step=100,
                              help="å“åº”ä¸­çš„æœ€å¤§æ ‡è®°æ•°")
    
    st.divider()
    st.markdown("### å…³äº ###")
    st.markdown("è™šå‡æ–°é—»æ£€æµ‹å™¨:")
    st.markdown("1. ä»æ–°é—»ä¸­æå–æ ¸å¿ƒå£°æ˜")
    st.markdown("2. åœ¨ç½‘ç»œä¸Šå’Œæœ¬åœ°åº“æœç´¢è¯æ®")
    st.markdown("3. ä½¿ç”¨BGE-M3æŒ‰ç›¸å…³æ€§å¯¹è¯æ®è¿›è¡Œæ’å")
    st.markdown("4. åŸºäºè¯æ®æä¾›ç»“è®º")
    st.markdown("ä½¿ç”¨LLMã€Streamlitã€BGE-M3å’Œragå¼€å‘ â¤ï¸â¤ï¸â¤ï¸")

# å¦‚æœä¸å­˜åœ¨ï¼Œåˆå§‹åŒ–ä¼šè¯çŠ¶æ€ä»¥å­˜å‚¨èŠå¤©å†å²
if 'messages' not in st.session_state:
    st.session_state.messages = []

# æ˜¾ç¤ºèŠå¤©å†å²
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ä¸»è¾“å…¥åŒºåŸŸ
user_input = st.chat_input("è¯·åœ¨ä¸‹æ–¹è¾“å…¥éœ€è¦æ ¸æŸ¥çš„æ–°é—»...")

if user_input:
    # å°†ç”¨æˆ·æ¶ˆæ¯æ·»åŠ åˆ°èŠå¤©å†å²
    st.session_state.messages.append({"role": "user", "content": user_input})
    
    # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
    with st.chat_message("user"):
        st.markdown(user_input)
    
    # ä½¿ç”¨ with ä¸Šä¸‹æ–‡åˆ›å»ºåŠ©æ‰‹æ¶ˆæ¯å®¹å™¨
    with st.chat_message("assistant"):
        # åˆ›å»ºå¤šä¸ªç‹¬ç«‹å ä½ç¬¦ï¼Œé¿å… DOM é”™è¯¯
        claim_placeholder = st.empty()
        information_placeholder = st.empty()
        evidence_placeholder = st.empty()
        verdict_placeholder = st.empty()

        # åˆå§‹åŒ–FactChecker
        fact_checker = FactChecker(api_base, model_option, temperature, max_tokens)

        # ç¬¬1æ­¥ï¼šæå–å£°æ˜
        claim_placeholder.markdown("### ğŸ” æ­£åœ¨æå–æ–°é—»çš„æ ¸å¿ƒå£°æ˜...")
        claim = fact_checker.extract_claim(user_input)

        # å¤„ç†claimå­—ç¬¦ä¸²ï¼Œæå–"claim:"åé¢çš„å†…å®¹
        if "claim:" in claim.lower():
            claim = claim.split("claim:")[-1].strip()
        claim_placeholder.markdown(f"### ğŸ” æå–æ–°é—»çš„æ ¸å¿ƒå£°æ˜\n\n{claim}")

        # æå–å…³é”®ä¿¡æ¯
        information_placeholder.markdown("### ğŸ” æ­£åœ¨æå–æ–°é—»çš„å…³é”®ä¿¡æ¯...")
        information = fact_checker.extract_keyinformation(user_input)
        information_placeholder.markdown(f"### ğŸ” æå–æ–°é—»çš„å…³é”®ä¿¡æ¯\n\n{information}")

        # ç¬¬2æ­¥ï¼šæœç´¢è¯æ®
        evidence_placeholder.markdown("### ğŸŒ æ­£åœ¨æœç´¢ç›¸å…³è¯æ®...")
        evidence_docs = fact_checker.search_evidence(claim)

        # ç¬¬3æ­¥ï¼šè·å–ç›¸å…³è¯æ®å—
        evidence_placeholder.markdown("### ğŸŒ æ­£åœ¨åˆ†æè¯æ®ç›¸å…³æ€§...")
        evidence_chunks = fact_checker.get_evidence_chunks(evidence_docs, claim)

        # æ˜¾ç¤ºè¯æ®ç»“æœ
        evidence_md = "### ğŸ”— è¯æ®æ¥æº\n\n"
        for j, chunk in enumerate(evidence_chunks[:-1]):  # è·³è¿‡æœ€åä¸€ä¸ªï¼Œä¸åŸå§‹ä»£ç ä¿æŒä¸€è‡´
            evidence_md += f"**[{j+1}]:**\n"
            evidence_md += f"{chunk['text']}\n"
            evidence_md += f"æ¥æº: {chunk['source']}\n\n"
        evidence_placeholder.markdown(evidence_md)

        # ç¬¬4æ­¥ï¼šè¯„ä¼°å£°æ˜
        verdict_placeholder.markdown("### âš–ï¸ æ­£åœ¨è¯„ä¼°å£°æ˜çœŸå®æ€§...")
        evaluation = fact_checker.evaluate_claim(information, user_input, evidence_chunks)

        # ç¡®å®šç»“è®ºè¡¨æƒ…ç¬¦å·
        verdict = evaluation["verdict"]
        if verdict.upper() == "TRUE":
            emoji = "âœ…"
            verdict_cn = "æ­£ç¡®"
        elif verdict.upper() == "FALSE":
            emoji = "âŒ"
            verdict_cn = "é”™è¯¯"
        elif verdict.upper() == "PARTIALLY TRUE":
            emoji = "âš ï¸"
            verdict_cn = "éƒ¨åˆ†æ­£ç¡®"
        else:
            emoji = "â“"
            verdict_cn = "æ— æ³•éªŒè¯"

        # æ˜¾ç¤ºæœ€ç»ˆç»“è®º
        verdict_md = f"### {emoji} ç»“è®º: {verdict_cn}\n\n"
        verdict_md += f"### æ¨ç†è¿‡ç¨‹\n\n{evaluation['reasoning']}\n\n"

        verdict_placeholder.markdown(verdict_md)

        # æ•´åˆå®Œæ•´çš„å“åº”å†…å®¹ç”¨äºä¿å­˜åˆ°èŠå¤©å†å²
        full_response = f"""
### ğŸ” æå–æ–°é—»çš„æ ¸å¿ƒå£°æ˜

{claim}

---

{information}

---

{evidence_md}

---

{verdict_md}
"""

        # æ·»åŠ åŠ©æ‰‹å“åº”åˆ°èŠå¤©å†å²
        st.session_state.messages.append({"role": "assistant", "content": full_response})
